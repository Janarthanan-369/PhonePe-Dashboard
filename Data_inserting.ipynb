{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9947bd71",
   "metadata": {},
   "source": [
    " Phone PE pulse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ad6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/PhonePe/pulse.git #dataset cloned from github using 'git clone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5aa44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.0.41)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sqlalchemy) (4.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72be5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.9.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b594e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.10 (dt dec pq3 ext lo64)\n",
      "2.3.1\n",
      "2.3.1\n",
      "2.0.41\n"
     ]
    }
   ],
   "source": [
    "# verifying using special variable\n",
    "import psycopg2\n",
    "print(psycopg2.__version__)\n",
    "\n",
    "import numpy\n",
    "print(numpy.__version__)  \n",
    "\n",
    "import pandas\n",
    "print(pandas.__version__)  \n",
    "\n",
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a231ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #'OS' module for System operations like file handling\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ae7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 0: Set the root path to the 'state' folder\n",
    "root_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/aggregated/insurance/country/india/state'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4bba9",
   "metadata": {},
   "source": [
    "Looping into folders & extracts the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31eb1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved at: /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/aggregated_insurance.csv\n"
     ]
    }
   ],
   "source": [
    "# Make a list to store all the data\n",
    "data_rows = []\n",
    "\n",
    "# Go through each state folder\n",
    "for state in os.listdir(root_path):\n",
    "    state_path = os.path.join(root_path, state)\n",
    "    if os.path.isdir(state_path):  # Only look at folders\n",
    "        # Go through each year folder\n",
    "        for year in os.listdir(state_path):\n",
    "            year_path = os.path.join(state_path, year)\n",
    "            if os.path.isdir(year_path):\n",
    "                # Go through each quarter file\n",
    "                for file in os.listdir(year_path):\n",
    "                    if file.endswith(\".json\"):\n",
    "                        file_path = os.path.join(year_path, file)\n",
    "                        quarter = int(file.replace(\".json\", \"\"))\n",
    "                        try:\n",
    "                            # Open and read the JSON file\n",
    "                            with open(file_path, \"r\") as f:\n",
    "                                data = json.load(f)\n",
    "                            # Get the list of transactions\n",
    "                            txns = data.get(\"data\", {}).get(\"transactionData\", [])\n",
    "                            # Find the insurance info\n",
    "                            ins = next((x for x in txns if x.get(\"name\") == \"Insurance\"), None)\n",
    "                            # If no insurance, add empty info\n",
    "                            if not ins:\n",
    "                                data_rows.append({\n",
    "                                    \"state\": state,\n",
    "                                    \"year\": int(year),\n",
    "                                    \"quarter\": quarter,\n",
    "                                    \"insurance_type\": None,\n",
    "                                    \"transaction_count\": None,\n",
    "                                    \"amount\": None\n",
    "                                })\n",
    "                            else:\n",
    "                                # Add info for each payment type\n",
    "                                for pay in ins.get(\"paymentInstruments\", []):\n",
    "                                    data_rows.append({\n",
    "                                        \"state\": state,\n",
    "                                        \"year\": int(year),\n",
    "                                        \"quarter\": quarter,\n",
    "                                        \"insurance_type\": pay.get(\"type\"),\n",
    "                                        \"transaction_count\": pay.get(\"count\"),\n",
    "                                        \"amount\": pay.get(\"amount\")\n",
    "                                    })\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Skipped {file_path}: {e}\")\n",
    "\n",
    "# Make a DataFrame from the list\n",
    "df = pd.DataFrame(data_rows)\n",
    "\n",
    "# Remove rows with missing important info\n",
    "df = df.dropna(subset=['transaction_count', 'amount', 'year', 'quarter'])\n",
    "\n",
    "# Save the table as a CSV file\n",
    "output_folder = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv\"\n",
    "output_csv_path = os.path.join(output_folder, \"aggregated_insurance.csv\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"✅ CSV saved at: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5374b",
   "metadata": {},
   "source": [
    "inserting into SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e3d580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load CSV\n",
    "csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/aggregated_insurance.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 2: Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "print(\"✅ Connected to PostgreSQL.\")\n",
    "\n",
    "# Step 3: Create table if not exists\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS aggregated_insurance (\n",
    "    state TEXT,\n",
    "    year INTEGER,\n",
    "    quarter INTEGER,\n",
    "    insurance_type TEXT,\n",
    "    transaction_count BIGINT,\n",
    "    amount DOUBLE PRECISION\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Step 4: Insert rows\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO aggregated_insurance (state, year, quarter, insurance_type, transaction_count, amount)\n",
    "VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        cur.execute(insert_query, tuple(row))\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error row:\", row)\n",
    "        print(\"❌ Error:\", e)\n",
    "        break  # stop on first error\n",
    "\n",
    "\n",
    "# Step 5: Close connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6eeec",
   "metadata": {},
   "source": [
    "Aggregated Transcations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7164f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CSV (PostgreSQL-ready) exported to:\n",
      "/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/aggregated_transactions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === 🔧 CONFIGURATION ===\n",
    "BASE_PATH = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/aggregated/transaction/country/india/state'\n",
    "OUTPUT_CSV = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/aggregated_transactions.csv'\n",
    "\n",
    "# === 📥 Data Collector ===\n",
    "data_rows = []\n",
    "\n",
    "# Walk through all state folders\n",
    "for state_folder in os.listdir(BASE_PATH):\n",
    "    state_path = os.path.join(BASE_PATH, state_folder)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year_folder in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year_folder)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for quarter_file in os.listdir(year_path):\n",
    "            if not quarter_file.endswith('.json'):\n",
    "                continue\n",
    "\n",
    "            quarter = int(quarter_file.replace('Q', '').replace('.json', ''))  # e.g., Q1.json → 1\n",
    "            file_path = os.path.join(year_path, quarter_file)\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                try:\n",
    "                    content = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"❌ Invalid JSON in {file_path}\")\n",
    "                    continue\n",
    "\n",
    "            data = content.get(\"data\", {})\n",
    "            transaction_data = data.get(\"transactionData\", [])\n",
    "            from_ts = data.get(\"from\")\n",
    "            to_ts = data.get(\"to\")\n",
    "            response_ts = content.get(\"responseTimestamp\")\n",
    "\n",
    "            if not from_ts or not to_ts:\n",
    "                continue\n",
    "\n",
    "            for txn_entry in transaction_data:\n",
    "                category = txn_entry.get(\"name\")\n",
    "                instruments = txn_entry.get(\"paymentInstruments\", [])\n",
    "\n",
    "                for instrument in instruments:\n",
    "                    row = {\n",
    "                        \"state\": state_folder,\n",
    "                        \"year\": int(year_folder),\n",
    "                        \"quarter\": quarter,\n",
    "                        \"category\": category,\n",
    "                        \"txn_type\": instrument.get(\"type\"),\n",
    "                        \"txn_count\": instrument.get(\"count\"),\n",
    "                        \"txn_amount\": instrument.get(\"amount\"),                       \n",
    "                    }\n",
    "                    data_rows.append(row)\n",
    "\n",
    "# === 💾 Save to CSV for PostgreSQL ===\n",
    "if not data_rows:\n",
    "    print(\"⚠️ No data found!\")\n",
    "else:\n",
    "    df = pd.DataFrame(data_rows)\n",
    "\n",
    "    # Reorder columns to match PostgreSQL table\n",
    "    df = df[\n",
    "        [\n",
    "            \"state\", \"year\", \"quarter\", \"category\",\n",
    "            \"txn_type\", \"txn_count\", \"txn_amount\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Convert all fields to string to avoid type mismatches\n",
    "    df = df.astype(str)\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    output_dir = Path(OUTPUT_CSV).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df.to_csv(OUTPUT_CSV, index=False, header=True)  \n",
    "    print(f\"✅ Cleaned CSV (PostgreSQL-ready) exported to:\\n{OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826eb33",
   "metadata": {},
   "source": [
    "Connect to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c273423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL.\n",
      "🧱 Table checked/created.\n",
      "📥 Data inserted successfully.\n",
      "🔒 Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "# === 🛠 PostgreSQL Connection Config ===\n",
    "conn = None\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"1234\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    print(\"✅ Connected to PostgreSQL.\")\n",
    "\n",
    "    # === 📦 Create table if it doesn't exist\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS aggregated_transactions (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            state TEXT,\n",
    "            year INTEGER,\n",
    "            quarter INTEGER,\n",
    "            category TEXT,\n",
    "            txn_type TEXT,\n",
    "            txn_count BIGINT,\n",
    "            txn_amount DOUBLE PRECISION\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(\"🧱 Table checked/created.\")\n",
    "\n",
    "    # === 📤 Insert from CSV\n",
    "    with open('/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/aggregated_transactions.csv', 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO aggregated_transactions (\n",
    "                    state, year, quarter,\n",
    "                    category, txn_type,\n",
    "                    txn_count, txn_amount\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\", (\n",
    "                row['state'],\n",
    "                int(row['year']),\n",
    "                int(row['quarter']),\n",
    "                row['category'],\n",
    "                row['txn_type'],\n",
    "                int(row['txn_count']) if row['txn_count'] else 0,\n",
    "                float(row['txn_amount']) if row['txn_amount'] else 0.0,\n",
    "            ))\n",
    "\n",
    "    conn.commit()\n",
    "    print(\"📥 Data inserted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error during DB operation:\", e)\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print(\"🔒 Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61761d",
   "metadata": {},
   "source": [
    "Aggregated Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cd1d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file 'aggregated_user.csv' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Paths\n",
    "input_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/aggregated/user/country/india/state\"\n",
    "output_csv = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/aggregated_user.csv\"\n",
    "\n",
    "# Output container\n",
    "final_data = []\n",
    "\n",
    "def safe_extract(json_data, key, default=None):\n",
    "    \"\"\"Safely extract nested values.\"\"\"\n",
    "    return json_data.get('data', {}).get(key, default)\n",
    "\n",
    "# Traverse all states\n",
    "for state in os.listdir(input_path):\n",
    "    state_path = os.path.join(input_path, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "\n",
    "        for file in os.listdir(year_path):\n",
    "            if file.endswith('.json'):\n",
    "                quarter = file.replace('.json', '')\n",
    "                file_path = os.path.join(year_path, file)\n",
    "\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    aggregated = safe_extract(data, 'aggregated', {})\n",
    "                    registered_users = aggregated.get('registeredUsers', 0)\n",
    "                    app_opens = aggregated.get('appOpens', 0)\n",
    "\n",
    "                    devices = safe_extract(data, 'usersByDevice', [])\n",
    "\n",
    "                    if devices:\n",
    "                        for device in devices:\n",
    "                            row = {\n",
    "                                'state': state,\n",
    "                                'year': int(year),\n",
    "                                'quarter': int(quarter),\n",
    "                                'registered_users': registered_users,\n",
    "                                'app_opens': app_opens,\n",
    "                                'device_brand': device.get('brand'),\n",
    "                                'device_count': device.get('count'),\n",
    "                                'device_percentage': device.get('percentage'),\n",
    "                            }\n",
    "                            final_data.append(row)\n",
    "                    else:\n",
    "                        # Still log row even if usersByDevice is None or empty\n",
    "                        row = {\n",
    "                            'state': state,\n",
    "                            'year': int(year),\n",
    "                            'quarter': int(quarter),\n",
    "                            'registered_users': registered_users,\n",
    "                            'app_opens': app_opens,\n",
    "                            'device_brand': None,\n",
    "                            'device_count': None,\n",
    "                            'device_percentage': None,\n",
    "                        }\n",
    "                        final_data.append(row)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error reading {file_path}: {e}\")\n",
    "\n",
    "# Write CSV\n",
    "with open(output_csv, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        'state', 'year', 'quarter',\n",
    "        'registered_users', 'app_opens',\n",
    "        'device_brand', 'device_count', 'device_percentage'\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(final_data)\n",
    "\n",
    "print(\"✅ CSV file 'aggregated_user.csv' generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdb627",
   "metadata": {},
   "source": [
    "Invert into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb3b51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data inserted into PostgreSQL table 'aggregated_user'.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "# PostgreSQL connection\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Step 1: Create table\n",
    "cur.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS aggregated_user;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS aggregated_user (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    state TEXT,\n",
    "    year INT,\n",
    "    quarter INT,\n",
    "    registered_users BIGINT,\n",
    "    app_opens BIGINT,\n",
    "    device_brand TEXT,\n",
    "    device_count BIGINT,\n",
    "    device_percentage FLOAT\n",
    ");\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Step 2: Insert data from CSV\n",
    "with open('/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/aggregated_user.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO aggregated_user (\n",
    "                state, year, quarter,\n",
    "                registered_users, app_opens,\n",
    "                device_brand, device_count, device_percentage\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\n",
    "        \"\"\", (\n",
    "            row['state'],\n",
    "            int(row['year']),\n",
    "            int(row['quarter']),\n",
    "            int(row['registered_users']) if row['registered_users'] else 0,\n",
    "            int(row['app_opens']) if row['app_opens'] else 0,\n",
    "            row['device_brand'],\n",
    "            int(row['device_count']) if row['device_count'] else None,\n",
    "            float(row['device_percentage']) if row['device_percentage'] else None,\n",
    "        ))\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ Data inserted into PostgreSQL table 'aggregated_user'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b2b41",
   "metadata": {},
   "source": [
    "Map - insurance - Country/india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10cabdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved to: /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/insurance_hover.csv\n",
      "📊 Total rows written: 13876\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state_name       0\n",
      "year             0\n",
      "quarter          0\n",
      "district_name    0\n",
      "metric_type      0\n",
      "policy_count     0\n",
      "amount           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/map/insurance/hover/country/india/state\"\n",
    "output_csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/insurance_hover.csv\"\n",
    "\n",
    "hover_data_rows = []\n",
    "\n",
    "for state in os.listdir(base_path):\n",
    "    state_path = os.path.join(base_path, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(year_path):\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            quarter = file.replace(\".json\", \"\")\n",
    "            file_path = os.path.join(year_path, file)\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = json.load(f)\n",
    "\n",
    "                hover_list = content.get(\"data\", {}).get(\"hoverDataList\", [])\n",
    "\n",
    "                for item in hover_list:\n",
    "                    district = item.get(\"name\")\n",
    "                    metrics = item.get(\"metric\", [])\n",
    "\n",
    "                    for metric in metrics:\n",
    "                        hover_data_rows.append({\n",
    "                            \"state_name\": state,\n",
    "                            \"year\": int(year),\n",
    "                            \"quarter\": int(quarter),\n",
    "                            \"district_name\": district,\n",
    "                            \"metric_type\": metric.get(\"type\"),\n",
    "                            \"policy_count\": metric.get(\"count\"),\n",
    "                            \"amount\": metric.get(\"amount\")\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to parse: {file_path} → {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(hover_data_rows)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"✅ CSV saved to: {output_csv_path}\")\n",
    "print(\"📊 Total rows written:\", len(df))\n",
    "\n",
    "# Optional: Show null counts\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8582d",
   "metadata": {},
   "source": [
    "Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba14e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL.\n",
      "📦 Table created: insurance_hover\n",
      "✅ All rows inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv(\"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/insurance_hover.csv\")\n",
    "\n",
    "# PostgreSQL Connection Info\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "print(\"✅ Connected to PostgreSQL.\")\n",
    "\n",
    "# Optional: Drop & recreate table for fresh import (use with caution)\n",
    "cur.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS insurance_hover;\n",
    "    CREATE TABLE insurance_hover (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        state_name TEXT,\n",
    "        year INTEGER,\n",
    "        quarter INTEGER,\n",
    "        district_name TEXT,\n",
    "        metric_type TEXT,\n",
    "        policy_count INTEGER,\n",
    "        amount FLOAT\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"📦 Table created: insurance_hover\")\n",
    "\n",
    "# Insert rows\n",
    "for _, row in df.iterrows():\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO insurance_hover (\n",
    "            state_name, year, quarter, district_name, metric_type, policy_count, amount\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (\n",
    "        row['state_name'],\n",
    "        int(row['year']),\n",
    "        int(row['quarter']),\n",
    "        row['district_name'],\n",
    "        row['metric_type'],\n",
    "        int(row['policy_count']) if not pd.isnull(row['policy_count']) else None,\n",
    "        float(row['amount']) if not pd.isnull(row['amount']) else None\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "print(\"✅ All rows inserted successfully.\")\n",
    "\n",
    "# Close connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8197bf0",
   "metadata": {},
   "source": [
    "Map - Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd4779b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved to: /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/map_transaction_hover.csv\n",
      "📊 Total rows written: 20604\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state_name           0\n",
      "year                 0\n",
      "quarter              0\n",
      "district_name        0\n",
      "metric_type          0\n",
      "transaction_count    0\n",
      "amount               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "base_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/map/transaction/hover/country/india/state'\n",
    "output_csv = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/map_transaction_hover.csv'\n",
    "\n",
    "trans_data_rows = []\n",
    "\n",
    "for state in os.listdir(base_path):\n",
    "    state_path = os.path.join(base_path, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(year_path):\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            quarter = file.replace(\".json\", \"\")\n",
    "            file_path = os.path.join(year_path, file)\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = json.load(f)\n",
    "\n",
    "                hover_list = content.get(\"data\", {}).get(\"hoverDataList\", [])\n",
    "\n",
    "                for item in hover_list:\n",
    "                    district = item.get(\"name\")\n",
    "                    metrics = item.get(\"metric\", [])\n",
    "\n",
    "                    for metric in metrics:\n",
    "                        trans_data_rows.append({\n",
    "                            \"state_name\": state,\n",
    "                            \"year\": int(year),\n",
    "                            \"quarter\": int(quarter),\n",
    "                            \"district_name\": district,\n",
    "                            \"metric_type\": metric.get(\"type\"),\n",
    "                            \"transaction_count\": metric.get(\"count\"),\n",
    "                            \"amount\": metric.get(\"amount\")\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to parse: {file_path} → {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(trans_data_rows)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV saved to: {output_csv}\")\n",
    "print(\"📊 Total rows written:\", len(df))\n",
    "\n",
    "# Null values summary\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f67e4",
   "metadata": {},
   "source": [
    "Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01afe391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL\n",
      "✅ 20604 rows inserted into map_transaction_hover\n",
      "🔒 PostgreSQL connection closed\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV\n",
    "csv_file = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/map_transaction_hover.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"1234\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    print(\"✅ Connected to PostgreSQL\")\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS map_transaction_hover (\n",
    "            state_name TEXT,\n",
    "            year INTEGER,\n",
    "            quarter INTEGER,\n",
    "            district_name TEXT,\n",
    "            metric_type TEXT,\n",
    "            transaction_count BIGINT,\n",
    "            amount DOUBLE PRECISION\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert data row by row\n",
    "    for _, row in df.iterrows():\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO map_transaction_hover (\n",
    "                state_name, year, quarter, district_name, metric_type,\n",
    "                transaction_count, amount\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "            row['state_name'], row['year'], row['quarter'],\n",
    "            row['district_name'], row['metric_type'],\n",
    "            row['transaction_count'], row['amount']\n",
    "        ))\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"✅ {len(df)} rows inserted into map_transaction_hover\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(\"🔒 PostgreSQL connection closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c0689",
   "metadata": {},
   "source": [
    "Map-hover-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9cd87cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data saved to /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/map_user.csv\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state_name          0\n",
      "year                0\n",
      "quarter             0\n",
      "district_name       0\n",
      "registered_users    0\n",
      "app_opens           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output paths\n",
    "base_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/map/user/hover/country/india/state'\n",
    "output_csv = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/map_user.csv'\n",
    "\n",
    "user_data_rows = []\n",
    "\n",
    "for state in os.listdir(base_path):\n",
    "    state_path = os.path.join(base_path, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(year_path):\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            quarter = file.replace(\".json\", \"\")\n",
    "            file_path = os.path.join(year_path, file)\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    hover_data = data.get(\"data\", {}).get(\"hoverData\", {})\n",
    "\n",
    "                    for district, metrics in hover_data.items():\n",
    "                        user_data_rows.append({\n",
    "                            \"state_name\": state,\n",
    "                            \"year\": int(year),\n",
    "                            \"quarter\": int(quarter),\n",
    "                            \"district_name\": district,\n",
    "                            \"registered_users\": metrics.get(\"registeredUsers\"),\n",
    "                            \"app_opens\": metrics.get(\"appOpens\")\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error parsing {file_path}: {e}\")\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(user_data_rows)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Data saved to {output_csv}\")\n",
    "\n",
    "# 🔍 Null Values Summary\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4afcd",
   "metadata": {},
   "source": [
    "Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3396598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL.\n",
      "📦 Table 'map_user' ready.\n",
      "✅ Data inserted into 'map_user' table.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "csv_file = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/map_user.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# PostgreSQL connection details\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "print(\"✅ Connected to PostgreSQL.\")\n",
    "\n",
    "# Create table\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS map_user (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        state_name TEXT,\n",
    "        year INT,\n",
    "        quarter INT,\n",
    "        district_name TEXT,\n",
    "        registered_users BIGINT,\n",
    "        app_opens BIGINT\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"📦 Table 'map_user' ready.\")\n",
    "\n",
    "# Insert data\n",
    "for _, row in df.iterrows():\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO map_user (\n",
    "            state_name, year, quarter, district_name, registered_users, app_opens\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (\n",
    "        row['state_name'],\n",
    "        int(row['year']),\n",
    "        int(row['quarter']),\n",
    "        row['district_name'],\n",
    "        int(row['registered_users']) if pd.notnull(row['registered_users']) else None,\n",
    "        int(row['app_opens']) if pd.notnull(row['app_opens']) else None\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"✅ Data inserted into 'map_user' table.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b03cd2",
   "metadata": {},
   "source": [
    "Top - Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdcf1e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pgeocode\n",
      "  Downloading pgeocode-0.5.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests (from pgeocode)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from pgeocode) (2.3.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from pgeocode) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->pgeocode) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->pgeocode) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->pgeocode) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->pgeocode) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->pgeocode)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->pgeocode)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->pgeocode)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->pgeocode)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading pgeocode-0.5.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests, pgeocode\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [pgeocode]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.7.14 charset_normalizer-3.4.2 idna-3.10 pgeocode-0.5.0 requests-2.32.4 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pgeocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab7be9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pincode-india (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pincode-india\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pincode-india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b156e077",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'postal_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS_Project/Phone_pe_pluse/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'postal_code'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m in_df = pd.read_csv(local_path, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m, dtype={\u001b[33m\"\u001b[39m\u001b[33mpostal_code\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m})\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Create a simple pincode to area mapping\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m pincode_to_area = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43min_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpostal_code\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, in_df[\u001b[33m'\u001b[39m\u001b[33mplace_name\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Optional: Full info mapping (place, state, lat/lon)\u001b[39;00m\n\u001b[32m     31\u001b[39m pincode_full_info = {\n\u001b[32m     32\u001b[39m     row[\u001b[33m\"\u001b[39m\u001b[33mpostal_code\u001b[39m\u001b[33m\"\u001b[39m]: {\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplace_name\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m\"\u001b[39m\u001b[33mplace_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m in_df.iterrows()\n\u001b[32m     39\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS_Project/Phone_pe_pluse/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS_Project/Phone_pe_pluse/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'postal_code'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import certifi\n",
    "import urllib.request\n",
    "from io import StringIO\n",
    "\n",
    "# # ✅ Step 0: Fetch IN.tsv securely and build lookup dictionary\n",
    "# ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "# url = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/IN.txt'\n",
    "# response = urllib.request.urlopen(url, context=ssl_context)\n",
    "# in_txt_data = response.read().decode(\"utf-8\")\n",
    "\n",
    "# in_df = pd.read_csv(StringIO(in_txt_data), sep=\"\\t\", dtype={\"postal_code\": str})\n",
    "# pincode_to_area = dict(zip(in_df['postal_code'], in_df['place_name']))\n",
    "\n",
    "# ✅ Function to get area name from pincode\n",
    "\n",
    "# ✅ Step 0: Load IN.txt locally and build lookup dictionary\n",
    "\n",
    "local_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/IN.txt'\n",
    "\n",
    "# Load the file\n",
    "in_df = pd.read_csv(local_path, sep=\"\\t\", dtype={\"postal_code\": str})\n",
    "\n",
    "# Create a simple pincode to area mapping\n",
    "pincode_to_area = dict(zip(in_df['postal_code'], in_df['place_name']))\n",
    "\n",
    "# Optional: Full info mapping (place, state, lat/lon)\n",
    "pincode_full_info = {\n",
    "    row[\"postal_code\"]: {\n",
    "        \"place_name\": row[\"place_name\"],\n",
    "        \"state_name\": row[\"state_name\"],\n",
    "        \"latitude\": row[\"latitude\"],\n",
    "        \"longitude\": row[\"longitude\"]\n",
    "    }\n",
    "    for _, row in in_df.iterrows()\n",
    "}\n",
    "\n",
    "def get_area_name(pincode):\n",
    "    try:\n",
    "        if pd.isna(pincode) or pincode == '':\n",
    "            return None\n",
    "        return pincode_to_area.get(str(pincode))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ✅ Base paths\n",
    "base_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/top/insurance/country/india/state'\n",
    "output_csv_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_insurance_with_area.csv'\n",
    "\n",
    "# ✅ Step 1: Extract data from JSON files\n",
    "insurance_rows = []\n",
    "\n",
    "for state in os.listdir(base_path):\n",
    "    state_path = os.path.join(base_path, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(year_path):\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            quarter = file.replace(\".json\", \"\")\n",
    "            file_path = os.path.join(year_path, file)\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            districts = data.get(\"data\", {}).get(\"districts\", [])\n",
    "            pincodes = data.get(\"data\", {}).get(\"pincodes\", [])\n",
    "\n",
    "            # Districts block\n",
    "            for district_data in districts:\n",
    "                insurance_rows.append({\n",
    "                    \"state_name\": state,\n",
    "                    \"year\": int(year),\n",
    "                    \"quarter\": int(quarter),\n",
    "                    \"district_name\": district_data.get(\"entityName\"),\n",
    "                    \"pincode\": None,\n",
    "                    \"metric_type\": district_data.get(\"metric\", {}).get(\"type\"),\n",
    "                    \"policy_count\": district_data.get(\"metric\", {}).get(\"count\"),\n",
    "                    \"amount\": district_data.get(\"metric\", {}).get(\"amount\")\n",
    "                })\n",
    "\n",
    "            # Pincodes block\n",
    "            for pincode_data in pincodes:\n",
    "                insurance_rows.append({\n",
    "                    \"state_name\": state,\n",
    "                    \"year\": int(year),\n",
    "                    \"quarter\": int(quarter),\n",
    "                    \"district_name\": None,\n",
    "                    \"pincode\": pincode_data.get(\"entityName\"),\n",
    "                    \"metric_type\": pincode_data.get(\"metric\", {}).get(\"type\"),\n",
    "                    \"policy_count\": pincode_data.get(\"metric\", {}).get(\"count\"),\n",
    "                    \"amount\": pincode_data.get(\"metric\", {}).get(\"amount\")\n",
    "                })\n",
    "\n",
    "# ✅ Step 2: Create DataFrame\n",
    "df = pd.DataFrame(insurance_rows)\n",
    "\n",
    "# ✅ Step 3: Add area_name using pincode\n",
    "df['area_name'] = df['pincode'].apply(get_area_name)\n",
    "\n",
    "# ✅ Step 4: Reorder columns → area_name next to district_name\n",
    "cols = df.columns.tolist()\n",
    "district_index = cols.index(\"district_name\")\n",
    "cols.remove(\"area_name\")\n",
    "cols.insert(district_index + 1, \"area_name\")\n",
    "df = df[cols]\n",
    "\n",
    "# ✅ Step 5: Save to CSV\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\n✅ {len(df)} records exported to → {output_csv_path}\")\n",
    "\n",
    "# ✅ Step 6: Null value summary\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "684aae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 12276 records exported to → /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_insurance_with_area.csv\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state_name          0\n",
      "year                0\n",
      "quarter             0\n",
      "district_name    6668\n",
      "area_name        5683\n",
      "pincode          5611\n",
      "metric_type         0\n",
      "policy_count        0\n",
      "amount              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ Step 0: Load all_india_pincodes.csv and build lookup dictionaries\n",
    "pincode_csv_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/all_india_pincodes.csv'\n",
    "pin_df = pd.read_csv(pincode_csv_path, dtype={\"pincode\": str})\n",
    "\n",
    "# Pincode → Office Name (Area Name)\n",
    "pincode_to_area = dict(zip(pin_df['pincode'], pin_df['office_name']))\n",
    "\n",
    "# Optional: Full info dictionary\n",
    "pincode_full_info = {\n",
    "    row[\"pincode\"]: {\n",
    "        \"office_name\": row[\"office_name\"],\n",
    "        \"district\": row[\"district\"],\n",
    "        \"state\": row[\"state\"],\n",
    "        \"lat\": row[\"lat\"],\n",
    "        \"lon\": row[\"lon\"]\n",
    "    }\n",
    "    for _, row in pin_df.iterrows()\n",
    "}\n",
    "\n",
    "# ✅ Function to get area name from pincode\n",
    "def get_area_name(pincode):\n",
    "    try:\n",
    "        if pd.isna(pincode) or pincode == '':\n",
    "            return None\n",
    "        return pincode_to_area.get(str(pincode))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ✅ Paths\n",
    "base_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/top/insurance/country/india/state'\n",
    "output_csv_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_insurance_with_area.csv'\n",
    "\n",
    "# ✅ Step 1: Traverse directories and extract data\n",
    "insurance_rows = []\n",
    "\n",
    "for state in os.listdir(base_path):\n",
    "    state_path = os.path.join(base_path, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(year_path):\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            quarter = file.replace(\".json\", \"\")\n",
    "            file_path = os.path.join(year_path, file)\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            districts = data.get(\"data\", {}).get(\"districts\", [])\n",
    "            pincodes = data.get(\"data\", {}).get(\"pincodes\", [])\n",
    "\n",
    "            # Districts block\n",
    "            for district_data in districts:\n",
    "                insurance_rows.append({\n",
    "                    \"state_name\": state,\n",
    "                    \"year\": int(year),\n",
    "                    \"quarter\": int(quarter),\n",
    "                    \"district_name\": district_data.get(\"entityName\"),\n",
    "                    \"pincode\": None,\n",
    "                    \"metric_type\": district_data.get(\"metric\", {}).get(\"type\"),\n",
    "                    \"policy_count\": district_data.get(\"metric\", {}).get(\"count\"),\n",
    "                    \"amount\": district_data.get(\"metric\", {}).get(\"amount\")\n",
    "                })\n",
    "\n",
    "            # Pincodes block\n",
    "            for pincode_data in pincodes:\n",
    "                insurance_rows.append({\n",
    "                    \"state_name\": state,\n",
    "                    \"year\": int(year),\n",
    "                    \"quarter\": int(quarter),\n",
    "                    \"district_name\": None,\n",
    "                    \"pincode\": pincode_data.get(\"entityName\"),\n",
    "                    \"metric_type\": pincode_data.get(\"metric\", {}).get(\"type\"),\n",
    "                    \"policy_count\": pincode_data.get(\"metric\", {}).get(\"count\"),\n",
    "                    \"amount\": pincode_data.get(\"metric\", {}).get(\"amount\")\n",
    "                })\n",
    "\n",
    "# ✅ Step 2: Convert to DataFrame\n",
    "df = pd.DataFrame(insurance_rows)\n",
    "\n",
    "# ✅ Step 3: Add area_name from pincode\n",
    "df['area_name'] = df['pincode'].apply(get_area_name)\n",
    "\n",
    "# ✅ Step 4: Reorder columns\n",
    "cols = df.columns.tolist()\n",
    "district_index = cols.index(\"district_name\")\n",
    "cols.remove(\"area_name\")\n",
    "cols.insert(district_index + 1, \"area_name\")\n",
    "df = df[cols]\n",
    "\n",
    "# ✅ Step 5: Export to CSV\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\n✅ {len(df)} records exported to → {output_csv_path}\")\n",
    "\n",
    "# ✅ Step 6: Null value summary\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f4628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/all_india_pincodes.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-edited the IN.txt file \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load and inspect the raw GeoNames IN.txt structure (manually confirmed earlier)\n",
    "columns = [\n",
    "    \"country_code\", \"postal_code\", \"place_name\", \"state_name\", \"state_code\",\n",
    "    \"county_name\", \"county_code\", \"community_name\", \"community_code\",\n",
    "    \"latitude\", \"longitude\", \"accuracy\"\n",
    "]\n",
    "\n",
    "# Load the cleaned IN.txt file as DataFrame\n",
    "df = pd.read_csv(\n",
    "    \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/IN.txt\",\n",
    "    sep=\"\\t\", header=None, names=columns, dtype={\"postal_code\": str}\n",
    ")\n",
    "\n",
    "# Filter only necessary columns and drop duplicates or missing postal codes\n",
    "df_clean = df[[\"postal_code\", \"place_name\", \"county_name\", \"state_name\", \"latitude\", \"longitude\"]]\n",
    "df_clean = df_clean.dropna(subset=[\"postal_code\"]).drop_duplicates(subset=[\"postal_code\"])\n",
    "\n",
    "# Rename for consistency\n",
    "df_clean.columns = [\"pincode\", \"office_name\", \"district\", \"state\", \"lat\", \"lon\"]\n",
    "\n",
    "# Save to a cleaned CSV\n",
    "output_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/all_india_pincodes.csv\"\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0dba7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Re-define columns since the state was reset\n",
    "columns = [\n",
    "    \"country_code\", \"postal_code\", \"place_name\", \"state_name\", \"state_code\",\n",
    "    \"county_name\", \"county_code\", \"community_name\", \"community_code\",\n",
    "    \"latitude\", \"longitude\", \"accuracy\"\n",
    "]\n",
    "\n",
    "# Load the file again\n",
    "file_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/IN.txt\"\n",
    "\n",
    "# Attempt to load and clean data\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=\"\\t\", header=None, names=columns, dtype={\"postal_code\": str}\n",
    "    )\n",
    "\n",
    "    df_clean = df[[\"postal_code\", \"place_name\", \"county_name\", \"state_name\", \"latitude\", \"longitude\"]]\n",
    "    df_clean = df_clean.dropna(subset=[\"postal_code\"]).drop_duplicates(subset=[\"postal_code\"])\n",
    "\n",
    "    # Rename columns\n",
    "    df_clean.columns = [\"pincode\", \"office_name\", \"district\", \"state\", \"lat\", \"lon\"]\n",
    "\n",
    "    # Save as downloadable CSV\n",
    "    output_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/all_india_pincodes.csv\"\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "    output_path\n",
    "\n",
    "except Exception as e:\n",
    "    str(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02c3c4",
   "metadata": {},
   "source": [
    "Into DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcc69777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL.\n",
      "✅ 12276 rows inserted into 'insurance_data'\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ Step 1: Load the CSV\n",
    "csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_insurance_with_area.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ✅ Step 2: Connect to PostgreSQL\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"1234\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    print(\"✅ Connected to PostgreSQL.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection error:\", e)\n",
    "    exit()\n",
    "\n",
    "# ✅ Step 3: Create table if not exists\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS insurance_data (\n",
    "    state_name TEXT,\n",
    "    year INTEGER,\n",
    "    quarter INTEGER,\n",
    "    district_name TEXT,\n",
    "    area_name TEXT,\n",
    "    pincode TEXT,\n",
    "    metric_type TEXT,\n",
    "    policy_count BIGINT,\n",
    "    amount DOUBLE PRECISION\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# ✅ Step 4: Insert DataFrame into the table\n",
    "from io import StringIO\n",
    "\n",
    "buffer = StringIO()\n",
    "df.to_csv(buffer, index=False, header=False)\n",
    "buffer.seek(0)\n",
    "\n",
    "try:\n",
    "    cur.copy_expert(\"\"\"\n",
    "        COPY insurance_data (state_name, year, quarter, district_name, area_name, pincode, metric_type, policy_count, amount)\n",
    "        FROM STDIN WITH CSV\n",
    "    \"\"\", buffer)\n",
    "    conn.commit()\n",
    "    print(f\"✅ {len(df)} rows inserted into 'insurance_data'\")\n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(\"❌ Insert error:\", e)\n",
    "\n",
    "# ✅ Step 5: Done — close connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6e171",
   "metadata": {},
   "source": [
    "Top - Transcations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a3d2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV exported successfully to:\n",
      "/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_transaction.csv\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state_name            0\n",
      "year                  0\n",
      "quarter               0\n",
      "district_name         0\n",
      "transaction_count     0\n",
      "transaction_amount    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/top/transaction/country/india/state\"\n",
    "data = []\n",
    "\n",
    "for state in os.listdir(folder_path):\n",
    "    state_path = os.path.join(folder_path, state)\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        for file in os.listdir(year_path):\n",
    "            if file.endswith('.json'):\n",
    "                quarter = int(file.replace('.json', ''))\n",
    "                with open(os.path.join(year_path, file), 'r') as f:\n",
    "                    content = json.load(f)\n",
    "                    try:\n",
    "                        districts = content['data']['districts']\n",
    "                        for district in districts:\n",
    "                            data.append({\n",
    "                                \"state_name\": state,\n",
    "                                \"year\": int(year),\n",
    "                                \"quarter\": quarter,\n",
    "                                \"district_name\": district[\"entityName\"],\n",
    "                                \"transaction_count\": district[\"metric\"][\"count\"],\n",
    "                                \"transaction_amount\": district[\"metric\"][\"amount\"]\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Skipping file: {file}, Error: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "output_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_transaction.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ CSV exported successfully to:\\n{output_path}\")\n",
    "\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1593e309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8296\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e333ee",
   "metadata": {},
   "source": [
    "Into DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44b5e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL.\n",
      "✅ Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load CSV\n",
    "csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_transaction.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 2: Connect to PostgreSQL\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"1234\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    print(\"✅ Connected to PostgreSQL.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)\n",
    "    exit()\n",
    "\n",
    "# Step 3: Create Table\n",
    "create_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS top_transaction (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    state_name TEXT,\n",
    "    year INT,\n",
    "    quarter INT,\n",
    "    district_name TEXT,\n",
    "    transaction_count BIGINT,\n",
    "    transaction_amount DOUBLE PRECISION\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_query)\n",
    "conn.commit()\n",
    "\n",
    "# Step 4: Insert Data\n",
    "for index, row in df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO top_transaction (\n",
    "            state_name, year, quarter, district_name, transaction_count, transaction_amount\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (\n",
    "        row['state_name'],\n",
    "        int(row['year']),\n",
    "        int(row['quarter']),\n",
    "        row['district_name'],\n",
    "        int(row['transaction_count']),\n",
    "        float(row['transaction_amount'])\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "print(\"✅ Data inserted successfully.\")\n",
    "\n",
    "# Step 5: Close\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8138aa",
   "metadata": {},
   "source": [
    "Top -User "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85e53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved to: /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_user_by_pincode.csv\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state               0\n",
      "district_name      70\n",
      "area_name          70\n",
      "pincode             0\n",
      "registeredUsers     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 📂 Paths\n",
    "json_folder = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/top/user/country/india/state\"\n",
    "csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_user_by_pincode.csv\"\n",
    "pincode_csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/all_india_pincodes.csv\"\n",
    "\n",
    "# ✅ Step 0: Load all_india_pincodes.csv and build lookup dictionary\n",
    "pin_df = pd.read_csv(pincode_csv_path, dtype={\"pincode\": str})\n",
    "\n",
    "pincode_info_map = {\n",
    "    row[\"pincode\"]: {\n",
    "        \"area_name\": row[\"office_name\"],\n",
    "        \"district\": row[\"district\"],\n",
    "        \"state\": row[\"state\"],\n",
    "        \"lat\": row.get(\"lat\"),\n",
    "        \"lon\": row.get(\"lon\")\n",
    "    }\n",
    "    for _, row in pin_df.iterrows()\n",
    "}\n",
    "\n",
    "def get_pincode_info(pincode):\n",
    "    if pd.isna(pincode) or pincode == '':\n",
    "        return {}\n",
    "    return pincode_info_map.get(str(pincode), {})\n",
    "\n",
    "# 📝 Records list\n",
    "records = []\n",
    "\n",
    "# 🔁 Traverse states > years > quarters\n",
    "for state in os.listdir(json_folder):\n",
    "    state_path = os.path.join(json_folder, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for quarter_file in os.listdir(year_path):\n",
    "            if not quarter_file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(year_path, quarter_file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    state_name_from_path = state.replace(\"-\", \" \").title()\n",
    "\n",
    "                    pincodes = data.get(\"data\", {}).get(\"pincodes\", [])\n",
    "                    for pincode_obj in pincodes:\n",
    "                        pincode = str(pincode_obj.get(\"name\"))\n",
    "                        registeredUsers = pincode_obj.get(\"registeredUsers\")\n",
    "\n",
    "                        info = get_pincode_info(pincode)\n",
    "                        records.append({\n",
    "                            \"state\": info.get(\"state\", state_name_from_path),\n",
    "                            \"district_name\": info.get(\"district\"),\n",
    "                            \"area_name\": info.get(\"area_name\"),\n",
    "                            \"pincode\": pincode,\n",
    "                            \"registeredUsers\": registeredUsers,\n",
    "                            # \"appOpens\": None\n",
    "                        })\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"❌ Failed to parse {file_path}\")\n",
    "\n",
    "# 📊 Save to CSV\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV saved to: {csv_path}\")\n",
    "\n",
    "# 🧮 Null Values Summary\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c513cab",
   "metadata": {},
   "source": [
    "Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "282082df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to PostgreSQL.\n",
      "✅ Table is ready.\n",
      "✅ 10000 records inserted into 'top_user_by_pincode'.\n",
      "🔒 Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/cleaned_csv/top_user_by_pincode.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"1234\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    print(\"✅ Connected to PostgreSQL.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS top_user_by_pincode (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    state TEXT,\n",
    "    district_name TEXT,\n",
    "    area_name TEXT,\n",
    "    pincode TEXT,\n",
    "    registeredUsers INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "print(\"✅ Table is ready.\")\n",
    "\n",
    "# Insert records\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO top_user_by_pincode (state, district_name, area_name, pincode, registeredUsers)\n",
    "VALUES (%s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    cur.execute(insert_query, (\n",
    "        row[\"state\"],\n",
    "        row[\"district_name\"],\n",
    "        row[\"area_name\"],\n",
    "        row[\"pincode\"],\n",
    "        int(row[\"registeredUsers\"]) if not pd.isna(row[\"registeredUsers\"]) else None\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "print(f\"✅ {len(df)} records inserted into 'top_user_by_pincode'.\")\n",
    "\n",
    "# Close connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"🔒 Connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbd38d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.3.1)\n",
      "Requirement already satisfied: packaging<26,>=20 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.3.1)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting protobuf<7,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.32.4)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (4.14.1)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from streamlit) (6.5.1)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema-4.25.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.48.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.26.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading jsonschema-4.25.0-py3-none-any.whl (89 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading narwhals-1.48.1-py3-none-any.whl (377 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.26.0-cp313-cp313-macosx_11_0_arm64.whl (350 kB)\n",
      "Installing collected packages: toml, tenacity, smmap, rpds-py, pyarrow, protobuf, pillow, narwhals, MarkupSafe, click, cachetools, blinker, attrs, referencing, jinja2, gitdb, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [streamlit]22\u001b[0m [streamlit]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 altair-5.5.0 attrs-25.3.0 blinker-1.9.0 cachetools-6.1.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.45 jinja2-3.1.6 jsonschema-4.25.0 jsonschema-specifications-2025.4.1 narwhals-1.48.1 pillow-11.3.0 protobuf-6.31.1 pyarrow-21.0.0 pydeck-0.9.1 referencing-0.36.2 rpds-py-0.26.0 smmap-5.0.2 streamlit-1.47.1 tenacity-9.1.2 toml-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d76788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from plotly) (1.48.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly-6.2.0-py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp313-cp313-macosx_11_0_arm64.whl (255 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, plotly, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]7\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 matplotlib-3.10.3 plotly-6.2.0 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7569c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watchdog\n",
      "  Downloading watchdog-6.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (44 kB)\n",
      "Downloading watchdog-6.0.0-cp313-cp313-macosx_11_0_arm64.whl (89 kB)\n",
      "Installing collected packages: watchdog\n",
      "Successfully installed watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install watchdog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc801406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydeck in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: jinja2>=2.10.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydeck) (3.1.6)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydeck) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2>=2.10.1->pydeck) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/macbook/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydeck pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a71313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved to: /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover.csv\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state                    0\n",
      "district                 0\n",
      "transaction_count        0\n",
      "transaction_amount       0\n",
      "latitude              5066\n",
      "longitude             5066\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === 📂 Input Paths ===\n",
    "json_folder = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/pulse/data/map/transaction/hover/country/india/state\"\n",
    "latlon_csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/GeoData/all_india_pincodes.csv\"\n",
    "\n",
    "# === 💾 Output Path ===\n",
    "output_csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover.csv\"\n",
    "\n",
    "# === 📍 Load Latitude & Longitude Data ===\n",
    "latlon_df = pd.read_csv(latlon_csv_path)\n",
    "\n",
    "# Normalize district names\n",
    "latlon_df['district_normalized'] = latlon_df['district'].str.strip().str.lower()\n",
    "\n",
    "# Build map from normalized district → (lat, lon)\n",
    "latlon_map = latlon_df.groupby('district_normalized')[['lat', 'lon']].first().to_dict(orient='index')\n",
    "\n",
    "def get_lat_lon(district_name):\n",
    "    if not district_name:\n",
    "        return (None, None)\n",
    "    normalized = district_name.strip().lower()\n",
    "    if normalized in latlon_map:\n",
    "        return latlon_map[normalized]['lat'], latlon_map[normalized]['lon']\n",
    "    \n",
    "    # Try partial match (fallback)\n",
    "    for district_key in latlon_map:\n",
    "        if normalized in district_key or district_key in normalized:\n",
    "            return latlon_map[district_key]['lat'], latlon_map[district_key]['lon']\n",
    "    return (None, None)\n",
    "\n",
    "# === 📊 Extract Data ===\n",
    "records = []\n",
    "\n",
    "for state in os.listdir(json_folder):\n",
    "    state_path = os.path.join(json_folder, state)\n",
    "    if not os.path.isdir(state_path):\n",
    "        continue\n",
    "\n",
    "    for year in os.listdir(state_path):\n",
    "        year_path = os.path.join(state_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for quarter_file in os.listdir(year_path):\n",
    "            if not quarter_file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(year_path, quarter_file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    hover_data = data.get(\"data\", {}).get(\"hoverDataList\", [])\n",
    "                    state_name = state.replace(\"-\", \" \").title()\n",
    "\n",
    "                    for entry in hover_data:\n",
    "                        district = entry.get(\"name\")\n",
    "                        metrics = entry.get(\"metric\", [])\n",
    "                        total_metric = next((m for m in metrics if m.get(\"type\") == \"TOTAL\"), {})\n",
    "                        count = total_metric.get(\"count\")\n",
    "                        amount = total_metric.get(\"amount\")\n",
    "\n",
    "                        lat, lon = get_lat_lon(district)\n",
    "\n",
    "                        records.append({\n",
    "                            \"state\": state_name,\n",
    "                            \"district\": district,\n",
    "                            \"transaction_count\": count,\n",
    "                            \"transaction_amount\": amount,\n",
    "                            \"latitude\": lat,\n",
    "                            \"longitude\": lon\n",
    "                        })\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON in file: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# === 📤 Save to CSV ===\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"✅ CSV saved to: {output_csv_path}\")\n",
    "\n",
    "# === 🧮 Null Values Summary ===\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3dd3509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting folium\n",
      "  Downloading folium-0.20.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Downloading branca-0.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from folium) (2.3.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from folium) (2.32.4)\n",
      "Collecting xyzservices (from folium)\n",
      "  Downloading xyzservices-2025.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->folium) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->folium) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->folium) (2025.7.14)\n",
      "Downloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\n",
      "Downloading branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Downloading xyzservices-2025.4.0-py3-none-any.whl (90 kB)\n",
      "Installing collected packages: xyzservices, branca, folium\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [folium]\n",
      "\u001b[1A\u001b[2KSuccessfully installed branca-0.8.1 folium-0.20.0 xyzservices-2025.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d28cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         column_name\n",
      "0            quarter\n",
      "1  transaction_count\n",
      "2             amount\n",
      "3               year\n",
      "4      district_name\n",
      "5         state_name\n",
      "6        metric_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/l1t8cgjd1v32y4nd2y9gl0580000gn/T/ipykernel_30822/2145687871.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_columns = pd.read_sql(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Connect to your PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# List columns in the table\n",
    "df_columns = pd.read_sql(\"\"\"\n",
    "    SELECT column_name \n",
    "    FROM information_schema.columns \n",
    "    WHERE table_name = 'map_transaction_hover'\n",
    "\"\"\", conn)\n",
    "\n",
    "print(df_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac279dca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'amount'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'amount'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Optional: format amount in Cr\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mformatted_amount\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m₹\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx/\u001b[32m1e7\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Cr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Basic 2D India map using lat/lon\u001b[39;00m\n\u001b[32m     11\u001b[39m fig = px.scatter_geo(\n\u001b[32m     12\u001b[39m     df,\n\u001b[32m     13\u001b[39m     lat=\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     title=\u001b[33m\"\u001b[39m\u001b[33mPhonePe Transactions Map (India)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'amount'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load your processed CSV file (with lat/lon already merged)\n",
    "df = pd.read_csv(\"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover.csv\")\n",
    "\n",
    "# Optional: format amount in Cr\n",
    "df[\"formatted_amount\"] = df[\"amount\"].apply(lambda x: f\"₹{x/1e7:.2f} Cr\")\n",
    "\n",
    "# Basic 2D India map using lat/lon\n",
    "fig = px.scatter_geo(\n",
    "    df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    scope=\"asia\",  # India falls under Asia\n",
    "    hover_name=\"district_name\",\n",
    "    hover_data={\n",
    "        \"state_name\": True,\n",
    "        \"transaction_count\": True,\n",
    "        \"formatted_amount\": True,\n",
    "        \"latitude\": False,\n",
    "        \"longitude\": False\n",
    "    },\n",
    "    color=\"amount\",\n",
    "    size=\"transaction_count\",\n",
    "    projection=\"natural earth\",\n",
    "    title=\"PhonePe Transactions Map (India)\"\n",
    ")\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "fig.update_layout(height=600, margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d0f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Year and Quarter added to CSV without modifying existing data.\n",
      "\n",
      "🧮 Null Values Summary:\n",
      "state_name           0\n",
      "district_name        0\n",
      "year             20604\n",
      "quarter          20604\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/l1t8cgjd1v32y4nd2y9gl0580000gn/T/ipykernel_48951/833484456.py:26: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sql_df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# === Load CSV ===\n",
    "csv_path = \"/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover.csv\"\n",
    "csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Rename CSV columns to match SQL for join\n",
    "csv_df = csv_df.rename(columns={\n",
    "    \"state\": \"state_name\",\n",
    "    \"district\": \"district_name\"\n",
    "})\n",
    "\n",
    "# === Connect to PostgreSQL and fetch year & quarter ===\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT state_name, district_name, year, quarter\n",
    "FROM map_transaction_hover;\n",
    "\"\"\"\n",
    "sql_df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Drop duplicates for clean merge\n",
    "sql_df = sql_df.drop_duplicates(subset=[\"state_name\", \"district_name\"])\n",
    "\n",
    "# === Merge without overwriting original CSV columns ===\n",
    "merged_df = pd.merge(csv_df, sql_df, on=[\"state_name\", \"district_name\"], how=\"left\")\n",
    "\n",
    "# === Save the updated CSV ===\n",
    "merged_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"✅ Year and Quarter added to CSV without modifying existing data.\")\n",
    "\n",
    "# # === Save back to CSV ===\n",
    "# merged_df.to_csv(csv_path, index=False)\n",
    "# print(f\"✅ CSV updated with year and quarter and saved to:\\n{csv_path}\")\n",
    "\n",
    "# === Optional: Show nulls if any\n",
    "print(\"\\n🧮 Null Values Summary:\")\n",
    "print(merged_df[[\"state_name\", \"district_name\", \"year\", \"quarter\"]].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf67c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated CSV with year and quarter saved to:\n",
      "/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover.csv\n",
      "\n",
      "🧮 Rows missing year/quarter:\n",
      "                           state                           district  \\\n",
      "0      Andaman & Nicobar Islands  north and middle andaman district   \n",
      "1      Andaman & Nicobar Islands             south andaman district   \n",
      "2      Andaman & Nicobar Islands                  nicobars district   \n",
      "3      Andaman & Nicobar Islands  north and middle andaman district   \n",
      "4      Andaman & Nicobar Islands             south andaman district   \n",
      "...                          ...                                ...   \n",
      "20599                   Nagaland                  tuensang district   \n",
      "20600                   Nagaland                     peren district   \n",
      "20601                   Nagaland                   dimapur district   \n",
      "20602                   Nagaland                 zunheboto district   \n",
      "20603                   Nagaland                     wokha district   \n",
      "\n",
      "       transaction_count  transaction_amount  latitude  longitude  year  \\\n",
      "0                  60059        1.616249e+08   20.4143    72.8324   NaN   \n",
      "1                 705512        1.872991e+09   20.4143    72.8324   NaN   \n",
      "2                  29177        9.441413e+07    7.5166    93.6031   NaN   \n",
      "3                  72573        1.956932e+08   20.4143    72.8324   NaN   \n",
      "4                 954875        2.357327e+09   20.4143    72.8324   NaN   \n",
      "...                  ...                 ...       ...        ...   ...   \n",
      "20599               5518        1.444341e+07   26.2217    94.8563   NaN   \n",
      "20600               2071        6.712395e+06   25.4996    93.6235   NaN   \n",
      "20601              68046        2.021721e+08   25.7785    93.7851   NaN   \n",
      "20602               4966        1.244903e+07       NaN        NaN   NaN   \n",
      "20603               4959        8.954847e+06   26.1849    94.1926   NaN   \n",
      "\n",
      "       quarter  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "3          NaN  \n",
      "4          NaN  \n",
      "...        ...  \n",
      "20599      NaN  \n",
      "20600      NaN  \n",
      "20601      NaN  \n",
      "20602      NaN  \n",
      "20603      NaN  \n",
      "\n",
      "[20604 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/l1t8cgjd1v32y4nd2y9gl0580000gn/T/ipykernel_48951/3666958161.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_db = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Step 1: Load backup CSV\n",
    "csv_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_2.csv'\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 2: Connect to PostgreSQL and fetch year & quarter\n",
    "conn = psycopg2.connect(\n",
    "    dbname='postgres',\n",
    "    user='postgres',\n",
    "    password='1234',\n",
    "    host='localhost',\n",
    "    port='5432'\n",
    ")\n",
    "\n",
    "query = '''\n",
    "SELECT \n",
    "    state_name AS state,\n",
    "    district_name AS district,\n",
    "    transaction_count,\n",
    "    amount AS transaction_amount,\n",
    "    year,\n",
    "    quarter\n",
    "FROM map_transaction_hover\n",
    "'''\n",
    "\n",
    "df_db = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Step 3: Merge based on state, district, transaction_count, and transaction_amount\n",
    "merged_df = pd.merge(\n",
    "    df_csv,\n",
    "    df_db,\n",
    "    on=['state', 'district', 'transaction_count', 'transaction_amount'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 4: Save updated CSV\n",
    "output_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Updated CSV with year and quarter saved to:\")\n",
    "print(output_path)\n",
    "\n",
    "print(\"\\n🧮 Rows missing year/quarter:\")\n",
    "print(merged_df[merged_df['year'].isnull()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db9719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated CSV saved at: /Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_2.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Add columns with null for now\n",
    "df[\"latitude\"] = None\n",
    "df[\"longitude\"] = None\n",
    "\n",
    "# Loop through rows and fetch latitude & longitude from SQL\n",
    "for idx, row in df.iterrows():\n",
    "    state = row['state']\n",
    "    district = row['district']\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT latitude, longitude\n",
    "        FROM map_transaction_hover\n",
    "        WHERE state_name = %s AND district_name = %s;\n",
    "    \"\"\", (state, district))\n",
    "\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        lat, lon = result\n",
    "        df.at[idx, \"latitude\"] = lat\n",
    "        df.at[idx, \"longitude\"] = lon\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ Updated CSV saved at: {csv_path}\")\n",
    "\n",
    "# Close DB\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f3c2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to the 'postgres' database.\n",
      "✅ Loaded 20604 rows from the 'map_transaction_hover' table.\n",
      "✅ Loaded 852 unique geo-locations from '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_3.csv'.\n",
      "\n",
      "Merging the database table with the CSV data...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/f4/l1t8cgjd1v32y4nd2y9gl0580000gn/T/ipykernel_48951/1027231217.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# We perform a 'left' merge to keep all records from your original database table\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# and add the lat/lon where the 'state' and 'district' match.\u001b[39;00m\n\u001b[32m     70\u001b[39m print(\u001b[33m\"\\nMerging the database table with the CSV data...\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m updated_df = pd.merge(db_df, geo_df, on=[\u001b[33m'state'\u001b[39m, \u001b[33m'district'\u001b[39m], how=\u001b[33m'left'\u001b[39m)\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Check if any rows failed to find a matching location\u001b[39;00m\n\u001b[32m     74\u001b[39m unmapped_rows = updated_df[\u001b[33m'latitude'\u001b[39m].isnull().sum()\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'state'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "\n",
    "# --- 1. DEFINE YOUR FILE PATHS AND DATABASE CREDENTIALS ---\n",
    "\n",
    "# The CSV file containing the latitude and longitude data\n",
    "CSV_PATH = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_3.csv'\n",
    "\n",
    "# Your PostgreSQL database connection details\n",
    "DB_CONFIG = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"1234\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"dbname\": \"postgres\"\n",
    "}\n",
    "\n",
    "# The name of the table you want to update\n",
    "TABLE_NAME = 'map_transaction_hover'\n",
    "\n",
    "# --- 2. CONNECT TO THE DATABASE AND LOAD THE TABLE ---\n",
    "\n",
    "try:\n",
    "    # Create a database connection URL and engine\n",
    "    db_url = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "    engine = create_engine(db_url)\n",
    "    \n",
    "    print(f\"✅ Successfully connected to the '{DB_CONFIG['dbname']}' database.\")\n",
    "    \n",
    "    # Load the entire 'map_transaction_hover' table into a DataFrame\n",
    "    db_df = pd.read_sql_table(TABLE_NAME, engine)\n",
    "    print(f\"✅ Loaded {len(db_df)} rows from the '{TABLE_NAME}' table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to the database or loading the table: {e}\")\n",
    "    sys.exit() # Exit the script if connection fails\n",
    "\n",
    "# --- 3. LOAD THE LATITUDE AND LONGITUDE DATA FROM THE CSV ---\n",
    "\n",
    "try:\n",
    "    # Read the CSV file\n",
    "    geo_df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # We only need the key columns ('state', 'district') and the geo data\n",
    "    # Assumption: Your CSV file has 'state' and 'district' columns that match the database table\n",
    "    required_csv_cols = ['state', 'district', 'latitude', 'longitude']\n",
    "    if not all(col in geo_df.columns for col in required_csv_cols):\n",
    "        print(f\"❌ CSV file is missing required columns. Expected: {required_csv_cols}\")\n",
    "        sys.exit()\n",
    "\n",
    "    geo_df = geo_df[required_csv_cols]\n",
    "    \n",
    "    # Remove any duplicate district entries to ensure a clean merge\n",
    "    geo_df.drop_duplicates(subset=['state', 'district'], inplace=True)\n",
    "    \n",
    "    print(f\"✅ Loaded {len(geo_df)} unique geo-locations from '{CSV_PATH}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file was not found at '{CSV_PATH}'. Please check the path.\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error reading the CSV file: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 4. MERGE THE TWO DATAFRAMES ---\n",
    "\n",
    "# We perform a 'left' merge to keep all records from your original database table\n",
    "# and add the lat/lon where the 'state' and 'district' match.\n",
    "print(\"\\nMerging the database table with the CSV data...\")\n",
    "updated_df = pd.merge(db_df, geo_df, on=['state', 'district'], how='left')\n",
    "\n",
    "# Check if any rows failed to find a matching location\n",
    "unmapped_rows = updated_df['latitude'].isnull().sum()\n",
    "if unmapped_rows > 0:\n",
    "    print(f\"⚠️ Warning: {unmapped_rows} rows in your database could not find a matching latitude/longitude.\")\n",
    "else:\n",
    "    print(\"✅ All rows were successfully mapped with geographic coordinates.\")\n",
    "\n",
    "# --- 5. WRITE THE UPDATED DATAFRAME BACK TO THE DATABASE ---\n",
    "\n",
    "try:\n",
    "    print(f\"\\nWriting the updated data back to the '{TABLE_NAME}' table...\")\n",
    "    \n",
    "    # This will DROP the old table and CREATE a new one with the merged data.\n",
    "    updated_df.to_sql(\n",
    "        TABLE_NAME,\n",
    "        engine,\n",
    "        if_exists='replace', # This replaces the entire table\n",
    "        index=False          # Do not write the DataFrame index as a column\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🎉 Success! Your database table has been updated with latitude and longitude values.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred while writing to the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03d80e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 🧐 Columns in your Database Table ---\n",
      "['state_name', 'year', 'quarter', 'district_name', 'metric_type', 'transaction_count', 'amount', 'latitude', 'longitude']\n",
      "\n",
      "--- 🧐 Columns in your CSV File ---\n",
      "['state', 'district', 'transaction_count', 'transaction_amount', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "\n",
    "# --- Your file paths and database credentials ---\n",
    "CSV_PATH = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_3.csv'\n",
    "DB_CONFIG = {\n",
    "    \"user\": \"postgres\", \"password\": \"1234\", \"host\": \"localhost\",\n",
    "    \"port\": \"5432\", \"dbname\": \"postgres\"\n",
    "}\n",
    "TABLE_NAME = 'map_transaction_hover'\n",
    "\n",
    "# --- Load data and print columns ---\n",
    "try:\n",
    "    # Load from Database\n",
    "    db_url = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "    engine = create_engine(db_url)\n",
    "    db_df = pd.read_sql_table(TABLE_NAME, engine)\n",
    "    print(\"--- 🧐 Columns in your Database Table ---\")\n",
    "    print(db_df.columns.tolist()) # This prints the DB column names\n",
    "\n",
    "    # Load from CSV\n",
    "    geo_df = pd.read_csv(CSV_PATH)\n",
    "    print(\"\\n--- 🧐 Columns in your CSV File ---\")\n",
    "    print(geo_df.columns.tolist()) # This prints the CSV column names\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0579300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 20604 rows from the 'map_transaction_hover' table.\n",
      "✅ Loaded 852 unique geo-locations from '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_3.csv'.\n",
      "\n",
      "Cleaning and standardizing state and district names for a reliable merge...\n",
      "✅ Join keys have been normalized.\n",
      "\n",
      "Merging the database table with the CSV data...\n",
      "⚠️ Warning: 10764 of 20604 rows could not find a matching location.\n",
      "\n",
      "Writing updated data back to 'map_transaction_hover'...\n",
      "\n",
      "🎉 Success! Your database table has been updated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "\n",
    "# --- 1. DEFINE YOUR FILE PATHS AND DATABASE CREDENTIALS ---\n",
    "CSV_PATH = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_3.csv'\n",
    "DB_CONFIG = {\n",
    "    \"user\": \"postgres\", \"password\": \"1234\", \"host\": \"localhost\",\n",
    "    \"port\": \"5432\", \"dbname\": \"postgres\"\n",
    "}\n",
    "TABLE_NAME = 'map_transaction_hover'\n",
    "\n",
    "# --- 2. CONNECT TO THE DATABASE AND LOAD THE TABLE ---\n",
    "try:\n",
    "    db_url = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "    engine = create_engine(db_url)\n",
    "    db_df = pd.read_sql_table(TABLE_NAME, engine)\n",
    "    print(f\"✅ Loaded {len(db_df)} rows from the '{TABLE_NAME}' table.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to the database: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. LOAD THE LATITUDE AND LONGITUDE DATA FROM THE CSV ---\n",
    "try:\n",
    "    geo_df = pd.read_csv(CSV_PATH)\n",
    "    geo_df = geo_df[['state', 'district', 'latitude', 'longitude']]\n",
    "    geo_df.drop_duplicates(subset=['state', 'district'], inplace=True)\n",
    "    print(f\"✅ Loaded {len(geo_df)} unique geo-locations from '{CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error reading or processing the CSV file: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# ⭐ --- 4. NEW: NORMALIZE JOIN KEYS FOR ACCURATE MERGING ---\n",
    "print(\"\\nCleaning and standardizing state and district names for a reliable merge...\")\n",
    "\n",
    "# Clean the database DataFrame columns\n",
    "for col in ['state_name', 'district_name']:\n",
    "    db_df[col] = db_df[col].str.lower().str.strip()\n",
    "\n",
    "# Clean the CSV DataFrame columns\n",
    "for col in ['state', 'district']:\n",
    "    geo_df[col] = geo_df[col].str.lower().str.strip()\n",
    "\n",
    "print(\"✅ Join keys have been normalized.\")\n",
    "\n",
    "# --- 5. MERGE THE TWO DATAFRAMES ---\n",
    "print(\"\\nMerging the database table with the CSV data...\")\n",
    "\n",
    "db_df_without_geo = db_df.drop(columns=['latitude', 'longitude'], errors='ignore')\n",
    "\n",
    "updated_df = pd.merge(\n",
    "    db_df_without_geo,\n",
    "    geo_df,\n",
    "    left_on=['state_name', 'district_name'],\n",
    "    right_on=['state', 'district'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "updated_df.drop(columns=['state', 'district'], inplace=True, errors='ignore')\n",
    "\n",
    "unmapped_rows = updated_df['latitude'].isnull().sum()\n",
    "if unmapped_rows > 0:\n",
    "    print(f\"⚠️ Warning: {unmapped_rows} of {len(updated_df)} rows could not find a matching location.\")\n",
    "else:\n",
    "    print(\"✅ All rows were successfully mapped with geographic coordinates.\")\n",
    "\n",
    "# --- 6. WRITE THE UPDATED DATAFRAME BACK TO THE DATABASE ---\n",
    "try:\n",
    "    print(f\"\\nWriting updated data back to '{TABLE_NAME}'...\")\n",
    "    updated_df.to_sql(TABLE_NAME, engine, if_exists='replace', index=False)\n",
    "    print(\"\\n🎉 Success! Your database table has been updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred while writing to the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "656b09ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Found 302 unique state/district pairs that could not be mapped.\n",
      "✅ A list of these has been saved to 'unmapped_districts.csv' for you to review.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "CSV_PATH = '/Users/macbook/Desktop/DS_Project/Phone_pe_pluse/PhonePe_Dashboard/map_transaction_hover_3.csv'\n",
    "DB_CONFIG = {\n",
    "    \"user\": \"postgres\", \"password\": \"1234\", \"host\": \"localhost\",\n",
    "    \"port\": \"5432\", \"dbname\": \"postgres\"\n",
    "}\n",
    "TABLE_NAME = 'map_transaction_hover'\n",
    "OUTPUT_FILE = 'unmapped_districts.csv' # The file this script will create\n",
    "\n",
    "# --- 2. LOAD AND CLEAN DATA ---\n",
    "try:\n",
    "    # Load from Database\n",
    "    db_url = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "    engine = create_engine(db_url)\n",
    "    db_df = pd.read_sql_table(TABLE_NAME, engine)\n",
    "\n",
    "    # Load from CSV\n",
    "    geo_df = pd.read_csv(CSV_PATH)\n",
    "    geo_df = geo_df[['state', 'district', 'latitude', 'longitude']]\n",
    "\n",
    "    # Normalize join keys\n",
    "    for col in ['state_name', 'district_name']:\n",
    "        db_df[col] = db_df[col].str.lower().str.strip()\n",
    "    for col in ['state', 'district']:\n",
    "        geo_df[col] = geo_df[col].str.lower().str.strip()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during file loading: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. PERFORM MERGE TO FIND MISMATCHES ---\n",
    "merged_df = pd.merge(\n",
    "    db_df,\n",
    "    geo_df,\n",
    "    left_on=['state_name', 'district_name'],\n",
    "    right_on=['state', 'district'],\n",
    "    how='left',\n",
    "    indicator=True # Adds a column to show the source of the match\n",
    ")\n",
    "\n",
    "# Filter for rows that are only in the database (no match in CSV)\n",
    "unmapped_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# --- 4. SAVE THE UNIQUE UNMAPPED NAMES TO A CSV FILE ---\n",
    "if not unmapped_df.empty:\n",
    "    # Get the unique list of unmapped state/district pairs\n",
    "    unique_unmapped = unmapped_df[['state_name', 'district_name']].drop_duplicates().sort_values(by=['state_name', 'district_name'])\n",
    "    \n",
    "    # Save the list to a new CSV file\n",
    "    unique_unmapped.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"⚠️ Found {len(unique_unmapped)} unique state/district pairs that could not be mapped.\")\n",
    "    print(f\"✅ A list of these has been saved to '{OUTPUT_FILE}' for you to review.\")\n",
    "else:\n",
    "    print(\"🎉 Great news! All rows were mapped successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
